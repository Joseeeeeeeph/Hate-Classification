{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "normalise = lambda x: x.lower().strip(' .').replace('\\'\\'', '\"').replace('``', '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, contents):\n",
    "        super(HateSpeechDataset, self).__init__()\n",
    "        self.contents = contents\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.contents[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.contents)\n",
    "\n",
    "dataset_path = 'data/hate-speech-dataset'\n",
    "all_files_path = os.path.join(dataset_path, 'all_files')\n",
    "train_path = os.path.join(dataset_path, 'sampled_train')\n",
    "test_path = os.path.join(dataset_path, 'sampled_test')\n",
    "path_list = [os.path.join(all_files_path, f) for f in os.listdir(all_files_path)]\n",
    "train_list = [f[:-4] for f in os.listdir(train_path)]\n",
    "test_list = [f[:-4] for f in os.listdir(test_path)]\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "data = pd.read_csv(os.path.join(dataset_path, 'annotations_metadata.csv'), usecols=['file_id', 'label']).values.tolist()\n",
    "for entry in data:\n",
    "    id = entry[0]\n",
    "    contents = normalise(open(os.path.join(all_files_path, f'{id}.txt'), mode='r', encoding='utf-8').read())\n",
    "    entry.append(contents)\n",
    "    if id in train_list:\n",
    "        train_data.append(tuple(entry[1:]))\n",
    "    elif id in test_list:\n",
    "        test_data.append(tuple(entry[1:]))\n",
    "\n",
    "training_dataset = HateSpeechDataset(train_data)\n",
    "testing_dataset = HateSpeechDataset(test_data)\n",
    "n = int(len(training_dataset) * 0.95)\n",
    "train_split, valid_split = random_split(training_dataset, [n, len(training_dataset) - n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.train(path_list, trainer=BpeTrainer(special_tokens=[\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[UNK]\"]))\n",
    "text_pipeline = lambda x: tokenizer.encode(x).ids\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(1 if _label == 'hate' else 0)\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "training_dataloader = DataLoader(train_split, batch_size=8, shuffle=True, collate_fn=collate_batch)\n",
    "validation_dataloader = DataLoader(valid_split, batch_size=8, shuffle=True, collate_fn=collate_batch)\n",
    "testing_dataloader = DataLoader(testing_dataset, batch_size=8, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model:\n",
      "end of epoch   1 | time:  3.08s | accuracy    0.708\n",
      "end of epoch   2 | time:  2.75s | accuracy    0.698\n",
      "end of epoch   3 | time:  2.80s | accuracy    0.708\n",
      "end of epoch   4 | time:  2.45s | accuracy    0.708\n",
      "end of epoch   5 | time:  2.45s | accuracy    0.708\n",
      "end of epoch   6 | time:  2.42s | accuracy    0.688\n",
      "end of epoch   7 | time:  2.41s | accuracy    0.698\n",
      "end of epoch   8 | time:  2.44s | accuracy    0.698\n",
      "end of epoch   9 | time:  2.64s | accuracy    0.698\n",
      "end of epoch  10 | time:  2.40s | accuracy    0.698\n",
      "\n",
      "Checking the results of test dataset.\n",
      "test accuracy    0.659\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "LR = 5.0\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "nClasses = len(set([x[0] for x in training_dataset]))\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "emsize = 64\n",
    "total_accuracy = None\n",
    "\n",
    "model = TextClassifier(vocab_size, emsize, nClasses).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time()\n",
    "\n",
    "    for id, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if id % log_interval == 0 and id > 0:\n",
    "            elapsed = time() - start_time\n",
    "            print('epoch {:3d} | {:5d}/{:5d} batches | accuracy {:8.3f}'.format(epoch, id, len(dataloader), total_acc / total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for id, (label, text, offsets) in enumerate(dataloader):\n",
    "            predited_label = model(text, offsets)\n",
    "            loss = criterion(predited_label, label)\n",
    "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count\n",
    "\n",
    "print('Training model:')\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time()\n",
    "    train(training_dataloader)\n",
    "    validation_accuracy = evaluate(validation_dataloader)\n",
    "    if total_accuracy is not None and total_accuracy > validation_accuracy:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accuracy = validation_accuracy\n",
    "    print('end of epoch {:3d} | time: {:5.2f}s | accuracy {:8.3f}'.format(epoch, time() - epoch_start_time, validation_accuracy))\n",
    "\n",
    "print(\"\\nChecking the results of test dataset.\")\n",
    "test_accuracy = evaluate(testing_dataloader)\n",
    "print(\"test accuracy {:8.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text), dtype=torch.int64)\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "    \n",
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I depise my own\" is hate\n"
     ]
    }
   ],
   "source": [
    "MESSAGE = input('Enter text to classify: ')\n",
    "\n",
    "print('\"' + MESSAGE + '\" is {}'.format('hate' if predict(normalise(MESSAGE), text_pipeline) == 1 else 'not hate'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
