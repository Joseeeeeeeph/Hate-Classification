\documentclass[12pt,a4paper]{article}
\usepackage{hyperref}
\usepackage{biblatex}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{booktabs}
\addbibresource{references.bib}
\titleformat*{\section}{\large\bfseries}
\graphicspath{ {./images/} }
\title{Hate Classification using Machine Learning}
\author{Joseph Adams}
\date{}
\begin{document}
\maketitle

\paragraph{Overview} This model is a ternary classification model that classifies text as either `hate', `maybe hate', or `not hate'. I decided to make it as a personal project to learn more about machine learning (as well as just for fun). None of the data used in the trainng of this model is owned by me.

\paragraph{Dataset} The model is trained on a dataset comprising of selected entries from 5 other datasets. Each entry is labelled as either 2 (hate), 1 (maybe hate), or 0 (not hate). The data used to create this dataset was taken from the following sources:
\begin{itemize}
    \item Automated Hate Speech Detection and the Problem of Offensive Language \cite{hateoffensive}
    \item Hate Speech Dataset from a White Supremacy Forum \cite{gibert2018hate}
    \item Constructing interval variables via faceted Rasch measurement and multitask deep learning: a hate speech application \cite{kennedy2020constructing}
    \item HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection \cite{mathew2021hatexplain}
    \item Large-Scale Hate Speech Detection with Cross-Domain Transfer \cite{toraman2022large}
\end{itemize}
The dataset comprises 137,343 entries. A stratified version of $k$-Folds cross-validation (where the class distribution remained consistent for each fold) was used to split the dataset into $k = 10$ folds for training and testing. The stratified version was used to address the class imbalance, where there are 84016 `not hate', 37827 `maybe hate', and 15500 `hate' entries. Each entry in the dataset was normalised with \texttt{normalise.py}, which made the text lowercase, removed most special characters (punctuation, emojis, unnecessary whitespace, etc.), and replaced various text segments to keep the data consistent. The text data was also written to individual files for each entry, for each dataset using \texttt{build.py} if it does not already exist, in order to provide a hard copy for the tokenisation process. The assembled data object created in \texttt{model.py} is stored as \textit{data.pkl} for faster access.

\paragraph{Neural Network Architecture} The model's neural network consists of an input layer; an embedding layer; two fully connected layers, each with batch normalisation, a \textsc{ReLU} activation function, and dropout; a third standalone fully connected layer, and an output layer. Xavier initialisation was used to initialise the weights of the network.

\paragraph{Performance} The model's performance was evaluated using cross entropy loss.

\printbibliography

\end{document}